{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import copy\n",
    "\n",
    "from torch_functions import *\n",
    "from MVCS import *\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (15129, 17) Y_train shape: (15129, 2)\n",
      "X_cal shape: (2161, 17) Y_cal shape: (2161, 2)\n",
      "X_test shape: (2162, 17) Y_test shape: (2162, 2)\n",
      "X_stop shape: (2161, 17) Y_stop shape: (2161, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def split_and_preprocess(X, Y, splits=[0.7, 0.1, 0.1, 0.1]):\n",
    "    \"\"\"\n",
    "    Sépare les données selon les proportions spécifiées et applique une normalisation par quantiles.\n",
    "    \n",
    "    Args:\n",
    "        X (numpy.ndarray): Features of shape (n_samples, d).\n",
    "        Y (numpy.ndarray): Outputs of shape (n_samples, k).\n",
    "        splits (list): Proportions of the dataset to use for training, validation, calibration and testing.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Un dictionnaire contenant les ensembles de données transformés.\n",
    "    \"\"\"\n",
    "    if not np.isclose(sum(splits), 1.0, atol=1e-10):\n",
    "        raise ValueError(\"Proportions need to sum to 1.0.\")\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    split_sizes = [int(n_samples * p ) for p in splits]\n",
    "    split_sizes[-1] = n_samples - sum(split_sizes[:-1])  \n",
    "        \n",
    "    indices = np.random.permutation(n_samples)\n",
    "    X, Y = X[indices], Y[indices]\n",
    "    \n",
    "    # Découper les ensembles selon les tailles définies\n",
    "    start = 0\n",
    "    subsets = {}\n",
    "    subset_names = [\"X_train\", \"X_stop\", \"X_calibration\", \"X_test\"] if len(splits) == 4 else [\"X_train\", \"X_calibration\", \"X_test\"]\n",
    "    \n",
    "    for i, name in enumerate(subset_names):\n",
    "        end = start + split_sizes[i]\n",
    "        subsets[name] = X[start:end]\n",
    "        subsets[name.replace(\"X_\", \"Y_\")] = Y[start:end]\n",
    "        start = end\n",
    "    \n",
    "    x_transformer = QuantileTransformer(output_distribution='normal')\n",
    "    subsets[\"X_train\"] = x_transformer.fit_transform(subsets[\"X_train\"])\n",
    "    for name in subset_names[1:]:  \n",
    "        subsets[name] = x_transformer.transform(subsets[name])\n",
    "\n",
    "    y_transformer = QuantileTransformer(output_distribution='normal')\n",
    "    subsets[\"Y_train\"] = y_transformer.fit_transform(subsets[\"Y_train\"])\n",
    "    for name in [n.replace(\"X_\", \"Y_\") for n in subset_names[1:]]:\n",
    "        subsets[name] = y_transformer.transform(subsets[name])\n",
    "\n",
    "    \n",
    "    return subsets\n",
    "\n",
    "\n",
    "load_path = \"../data/processed_data/house.npz\"\n",
    "data = np.load(load_path)\n",
    "X, Y = data['X'], data['Y']\n",
    "\n",
    "subsets = split_and_preprocess(X, Y, splits=[0.7, 0.1, 0.1, 0.1])\n",
    "x_train, y_train, x_calibration, y_calibration, x_test, y_test, x_stop, y_stop = subsets[\"X_train\"], subsets[\"Y_train\"], subsets[\"X_calibration\"], subsets[\"Y_calibration\"], subsets[\"X_test\"], subsets[\"Y_test\"], subsets[\"X_stop\"], subsets[\"Y_stop\"]\n",
    "\n",
    "dtype = torch.float64\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=dtype)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=dtype)\n",
    "x_stop_tensor = torch.tensor(x_stop, dtype=dtype)\n",
    "y_stop_tensor = torch.tensor(y_stop, dtype=dtype)\n",
    "x_calibration_tensor = torch.tensor(x_calibration, dtype=dtype)\n",
    "y_calibration_tensor = torch.tensor(y_calibration, dtype=dtype)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=dtype)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=dtype)\n",
    "\n",
    "\n",
    "print(\"X_train shape:\", x_train.shape, \"Y_train shape:\", y_train.shape)\n",
    "print(\"X_cal shape:\", x_calibration.shape, \"Y_cal shape:\", y_calibration.shape)\n",
    "print(\"X_test shape:\", x_test.shape, \"Y_test shape:\", y_test.shape)\n",
    "print(\"X_stop shape:\", x_stop.shape, \"Y_stop shape:\", y_stop.shape)\n",
    "\n",
    "d = x_train.shape[1]\n",
    "k = y_train.shape[1]\n",
    "\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare the variables and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, hidden_dim = 100, n_hidden_layers = 1):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_channels, hidden_dim)\n",
    "        self.tab_hidden = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(n_hidden_layers)])\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        for hidden in self.tab_hidden:\n",
    "            x = F.relu(hidden(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, train_loader, epochs, verbose=False):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        for epoch in range(epochs):\n",
    "            for i, (X_batch, Y_batch) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                output = self(X_batch)\n",
    "                loss = F.mse_loss(output, Y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if verbose:\n",
    "                print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')\n",
    "    \n",
    "    def fit(self, train_loader, stop_loader, epochs, lr = 0.001, verbose=False):\n",
    "        best_weights = None\n",
    "        best_loss = float('inf')\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        train_losses = []\n",
    "        stop_losses = []\n",
    "        for epoch in range(epochs):\n",
    "            for i, (X_batch, Y_batch) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                output = self(X_batch)\n",
    "                loss = F.mse_loss(output, Y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            train_loss = self.eval(train_loader)\n",
    "            stop_loss = self.eval(stop_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            stop_losses.append(stop_loss)\n",
    "            if stop_loss < best_loss:\n",
    "                best_loss = stop_loss\n",
    "                best_weights = copy.deepcopy(self.state_dict())\n",
    "            if verbose:\n",
    "                print(f'Epoch: {epoch + 1}, Train Loss: {train_loss}, Stop Loss: {stop_loss}')\n",
    "   \n",
    "        self.load_state_dict(best_weights)\n",
    "        return train_losses, stop_losses\n",
    "\n",
    "    def eval(self, loader):\n",
    "        with torch.no_grad():\n",
    "            loss = 0\n",
    "            for i, (X_batch, Y_batch) in enumerate(loader):\n",
    "                output = self(X_batch)\n",
    "                loss += F.mse_loss(output, Y_batch)\n",
    "            return loss / len(loader)\n",
    "\n",
    "class MatrixPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, output_rows, output_cols, hidden_dim = 100, n_hidden_layers = 0):\n",
    "        super(MatrixPredictor, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        if n_hidden_layers > 0:\n",
    "            self.tab_hidden = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(n_hidden_layers)])\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_rows * output_cols)\n",
    "        self.output_rows = output_rows\n",
    "        self.output_cols = output_cols\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        if self.n_hidden_layers > 0:\n",
    "            for hidden in self.tab_hidden:\n",
    "                x = F.relu(hidden(x))\n",
    "        output = self.fc2(x)\n",
    "        return output.view(-1, self.output_rows, self.output_cols)\n",
    "    \n",
    "model_center = Network(d, k, hidden_dim = 256, n_hidden_layers = 3).to(dtype)\n",
    "model_matrix = MatrixPredictor(d, k, k, hidden_dim = 256, n_hidden_layers = 3).to(dtype)\n",
    "\n",
    "model_center_init = copy.deepcopy(model_center)\n",
    "model_matrix_init = copy.deepcopy(model_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of Ellipsoid Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MVCSPredictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m calibrationloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTensorDataset(x_calibration_tensor, y_calibration_tensor), batch_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m testloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTensorDataset(x_test_tensor, y_test_tensor), batch_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m MVCS_predictor \u001b[38;5;241m=\u001b[39m MVCSPredictor(model_center, model_matrix)\n\u001b[1;32m     16\u001b[0m MVCS_predictor\u001b[38;5;241m.\u001b[39mfit(trainloader, \n\u001b[1;32m     17\u001b[0m                         stoploader, \n\u001b[1;32m     18\u001b[0m                         alpha,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m                         use_epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,          \u001b[38;5;66;03m# Add an epsilon to the loss to avoid numerical issues, usally not needed\u001b[39;00m\n\u001b[1;32m     29\u001b[0m                         )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MVCSPredictor' is not defined"
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor), batch_size= 100, shuffle=True)\n",
    "stoploader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_stop_tensor, y_stop_tensor), batch_size= 100, shuffle=True)\n",
    "\n",
    "\n",
    "# First, train the center model only with the strategy you want \n",
    "train_losses, stop_losses = model_center.fit(trainloader, stoploader, epochs=500, lr=0.0001)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor), batch_size= 100, shuffle=True)\n",
    "stoploader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_stop_tensor, y_stop_tensor), batch_size= 100, shuffle=True)\n",
    "calibrationloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_calibration_tensor, y_calibration_tensor), batch_size= 100, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test_tensor, y_test_tensor), batch_size= 100, shuffle=True)\n",
    "\n",
    "MVCS_predictor = MVCSPredictor(model_center, model_matrix)\n",
    "\n",
    "MVCS_predictor.fit(trainloader, \n",
    "                        stoploader, \n",
    "                        alpha,\n",
    "                        num_epochs = 200,             # The total number of epochs\n",
    "                        num_epochs_mat_only = 100,    # The first 100 epochs are used to train the matrix model only\n",
    "                        lr_model = 0.0001,            # The learning rate for the center model\n",
    "                        lr_q = 0.01,                  # The learning rate for q (the q-norm)\n",
    "                        lr_matrix_model = 0.004,      # The learning rate for the matrix model\n",
    "                        use_lr_scheduler = True,      # Use a learning rate scheduler (with cosine annealing)\n",
    "                        verbose = 1,                  # The verbosity level (0 : No verbose; 1: Print the loss 10 times or 2: Print the loss at each epoch)\n",
    "                        stop_on_best = True,          # Stop the training when the best model is found on a stop set (that is different from the calibration set and test set)\n",
    "                        loss_strategy = \"log_volume\", # The loss strategy to use (either the log volume or the exact volume)\n",
    "                        use_epsilon = False,          # Add an epsilon to the loss to avoid numerical issues, usally not needed\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Conformalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MVCS_predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m MVCS_predictor\u001b[38;5;241m.\u001b[39mconformalize(calibrationloader, alpha \u001b[38;5;241m=\u001b[39m alpha)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MVCS_predictor' is not defined"
     ]
    }
   ],
   "source": [
    "MVCS_predictor.conformalize(calibrationloader, alpha = alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MVCS_predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# For a given test set :\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ellipsoid_coverage \u001b[38;5;241m=\u001b[39m MVCS_predictor\u001b[38;5;241m.\u001b[39mget_coverage(testloader)\n\u001b[1;32m      3\u001b[0m ellipsoid_volume   \u001b[38;5;241m=\u001b[39m MVCS_predictor\u001b[38;5;241m.\u001b[39mget_averaged_volume(testloader)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe coverage on the test loader is \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m and the average volume is \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ellipsoid_coverage, ellipsoid_volume))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MVCS_predictor' is not defined"
     ]
    }
   ],
   "source": [
    "# For a given test set :\n",
    "ellipsoid_coverage = MVCS_predictor.get_coverage(testloader)\n",
    "ellipsoid_volume   = MVCS_predictor.get_averaged_volume(testloader)\n",
    "\n",
    "print(\"The coverage on the test loader is {:.2f} and the average volume is {:.2f}\\n\\n\".format(ellipsoid_coverage, ellipsoid_volume))\n",
    "\n",
    "# Or for some points \n",
    "x_to_test = x_test_tensor[:10]\n",
    "y_to_test = y_test_tensor[:10]\n",
    "\n",
    "in_ellipsoid = MVCS_predictor.is_inside(x_to_test, y_to_test)\n",
    "tab_volumes = MVCS_predictor.get_volumes(x_test=x_to_test)\n",
    "\n",
    "print(\"Are the points in the ellipsoid ?  : \", in_ellipsoid)\n",
    "print(\"The volumes of the set for each points are : \", tab_volumes) # The volumes of the set for each points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Get Lambdas and plot (if k=2) the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MVCS_predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m\n\u001b[1;32m     23\u001b[0m    \u001b[38;5;66;03m#  ax.scatter(*center, color='red', label='Center')\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ax\n\u001b[0;32m---> 28\u001b[0m f_x_test \u001b[38;5;241m=\u001b[39m MVCS_predictor\u001b[38;5;241m.\u001b[39mmodel(x_test_tensor)\n\u001b[1;32m     29\u001b[0m f_x_test_np \u001b[38;5;241m=\u001b[39m f_x_test\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     31\u001b[0m Lambdas_test \u001b[38;5;241m=\u001b[39m MVCS_predictor\u001b[38;5;241m.\u001b[39mget_Lambdas(x_test_tensor)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MVCS_predictor' is not defined"
     ]
    }
   ],
   "source": [
    "def add_ellipse(ax, center, Lambda, q, conformal_value, color='blue', linestyle = '-', label='Conformal Ellipse'):\n",
    "    \"\"\"\n",
    "       plot -> {y, \\| \\Lambda (u - center) \\|_q^q \\leq conformal_value}  }\n",
    "       conformal_value = radius ** q\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate a set of points on a unit circle\n",
    "    theta = np.linspace(0, 2 * np.pi, 500)\n",
    "    unit_circle = np.vstack((np.cos(theta), np.sin(theta)))\n",
    "    \n",
    "    # Scale the unit circle points to match the conformal value under the q-norm\n",
    "    scaling = conformal_value * np.power(np.sum(np.abs(unit_circle) ** q, axis=0), -1/q)\n",
    "    scaled_points = unit_circle * scaling\n",
    "    \n",
    "    # Apply the linear transformation Lambda\n",
    "    transformed_points = np.linalg.inv(Lambda) @ scaled_points\n",
    "    \n",
    "    # Translate the ellipse to the center\n",
    "    ellipse_points = transformed_points + np.reshape(center, (2, 1))\n",
    "    \n",
    "    # Add the ellipse to the existing plot\n",
    "    ax.plot(ellipse_points[0, :], ellipse_points[1, :], label=label, c=color, linestyle=linestyle)\n",
    "   #  ax.scatter(*center, color='red', label='Center')\n",
    "    \n",
    "    \n",
    "    return ax\n",
    "\n",
    "f_x_test = MVCS_predictor.center_model(x_test_tensor)\n",
    "f_x_test_np = f_x_test.detach().numpy()\n",
    "\n",
    "Lambdas_test = MVCS_predictor.get_Lambdas(x_test_tensor)\n",
    "Lambdas_test_np = Lambdas_test.detach().numpy()\n",
    "\n",
    "q_val = MVCS_predictor.q.item()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "color_map = plt.cm.get_cmap('viridis', 7)\n",
    "for i in range(7):    \n",
    "    ax.scatter(y_test[i, 0], y_test[i, 1],  c=color_map(i), alpha=0.5)\n",
    "    ax = add_ellipse(ax,  f_x_test_np[i], Lambdas_test_np[i], q_val, MVCS_predictor.nu_conformal.item(), color = color_map(i), label=None)    \n",
    "\n",
    "ax.grid(True)\n",
    "    \n",
    "ax.set_xlabel('y_1')\n",
    "ax.set_ylabel('y_2')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
